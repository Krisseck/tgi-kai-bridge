# generated by datamodel-codegen

from __future__ import annotations

from enum import Enum
from typing import Dict, List, Optional

from pydantic import BaseModel, Field, NonNegativeFloat, confloat, conint, constr


class AddSentenceSpacingSettings(BaseModel):
    value: bool


class AuthorsNoteDepthSetting(BaseModel):
    value: conint(ge=1, le=5)


class AuthorsNoteSetting(BaseModel):
    value: str


class AuthorsNoteTemplateSetting(BaseModel):
    value: str


class BasicBoolean(BaseModel):
    value: bool


class BasicBooleanResult(BaseModel):
    result: bool


class BasicError(BaseModel):
    msg: str
    type: str


class BasicResultInner(BaseModel):
    result: str


class BasicResults(BaseModel):
    results: List[BasicResultInner]


class BasicString(BaseModel):
    value: str


class BasicUID(BaseModel):
    uid: conint(ge=-2147483648, le=2147483647) = Field(
        ..., description='32-bit signed integer unique to this world info entry/folder.'
    )


class Empty(BaseModel):
    pass


class GenerationInput(BaseModel):
    disable_input_formatting: Optional[bool] = Field(
        True,
        description='When enabled, all input formatting options default to `false` instead of the value in the KoboldAI GUI',
    )
    disable_output_formatting: Optional[bool] = Field(
        True,
        description='When enabled, all output formatting options default to `false` instead of the value in the KoboldAI GUI.',
    )
    frmtadsnsp: Optional[bool] = Field(
        None,
        description='Input formatting option. When enabled, adds a leading space to your input if there is no trailing whitespace at the end of the previous action.\n\nIf `disable_input_formatting` is `true`, this defaults to `false` instead of the value in the KoboldAI GUI.',
    )
    frmtrmblln: Optional[bool] = Field(
        None,
        description='Output formatting option. When enabled, replaces all occurrences of two or more consecutive newlines in the output with one newline.\n\nIf `disable_output_formatting` is `true`, this defaults to `false` instead of the value in the KoboldAI GUI.',
    )
    frmtrmspch: Optional[bool] = Field(
        None,
        description='Output formatting option. When enabled, removes `#/@%{}+=~|\\^<>` from the output.\n\nIf `disable_output_formatting` is `true`, this defaults to `false` instead of the value in the KoboldAI GUI.',
    )
    frmttriminc: Optional[bool] = Field(
        None,
        description="Output formatting option. When enabled, removes some characters from the end of the output such that the output doesn't end in the middle of a sentence. If the output is less than one sentence long, does nothing.\n\nIf `disable_output_formatting` is `true`, this defaults to `false` instead of the value in the KoboldAI GUI.",
    )
    max_context_length: Optional[conint(ge=1)] = Field(
        None, description='Maximum number of tokens to send to the model.', examples=[2048]
    )
    max_length: Optional[conint(ge=1, le=512)] = Field(
        None, description='Number of tokens to generate.', examples=[80]
    )
    n: Optional[conint(ge=1, le=5)] = Field(
        None, description='Number of outputs to generate.', examples=[1]
    )
    prompt: str = Field(..., description='This is the submission.', examples=[ "Niko the kobold stalked carefully down the alley, his small scaly figure obscured by a dusky cloak that fluttered lightly in the cold winter breeze."])
    quiet: Optional[bool] = Field(
        None,
        description='When enabled, Generated output will not be displayed in the console.',
    )
    rep_pen: Optional[confloat(ge=1.0)] = Field(
        None, description='Base repetition penalty value.'
    )
    rep_pen_range: Optional[conint(ge=0)] = Field(
        None, description='Repetition penalty range.'
    )
    rep_pen_slope: Optional[confloat(ge=0.0)] = Field(
        None, description='Repetition penalty slope.'
    )
    sampler_full_determinism: Optional[bool] = Field(
        None,
        description='If enabled, the generated text will always be the same as long as you use the same RNG seed, input and settings. If disabled, only the *sequence* of generated texts that you get when repeatedly generating text will be the same given the same RNG seed, input and settings.',
    )
    sampler_order: Optional[List[int]] = Field(
        None,
        description='Sampler order to be used. If N is the length of this array, then N must be greater than or equal to 6 and the array must be a permutation of the first N non-negative integers.',
    )
    sampler_seed: Optional[conint(ge=0, le=18446744073709551615)] = Field(
        None,
        description='RNG seed to use for sampling. If not specified, the global RNG will be used.',
    )
    singleline: Optional[bool] = Field(
        None,
        description='Output formatting option. When enabled, removes everything after the first line of the output, including the newline.\n\nIf `disable_output_formatting` is `true`, this defaults to `false` instead of the value in the KoboldAI GUI.',
    )
    soft_prompt: Optional[constr(pattern=r'^[^/\\]*$')] = Field(
        None,
        description='Soft prompt to use when generating. If set to the empty string or any other string containing no non-whitespace characters, uses no soft prompt.',
    )
    stop_sequence: Optional[List[str]] = Field(
        None,
        description='An array of string sequences where the API will stop generating further tokens. The returned text WILL contain the stop sequence.',
        max_items=10,
    )
    temperature: Optional[NonNegativeFloat] = Field(None, description='Temperature value.', examples=[0.6])
    tfs: Optional[confloat(ge=0.0, le=1.0)] = Field(
        None, description='Tail free sampling value.'
    )
    top_a: Optional[confloat(ge=0.0)] = Field(None, description='Top-a sampling value.')
    top_k: Optional[conint(ge=0)] = Field(None, description='Top-k sampling value.')
    top_p: Optional[confloat(ge=0.0, le=1.0)] = Field(
        None, description='Top-p sampling value.', examples=[0.9]
    )
    typical: Optional[confloat(ge=0.0, le=1.0)] = Field(
        None, description='Typical sampling value.'
    )
    use_authors_note: Optional[bool] = Field(
        False,
        description="Whether or not to use the author's note from the KoboldAI GUI when generating text. This has no effect unless `use_story` is also enabled.",
    )
    use_memory: Optional[bool] = Field(
        False,
        description='Whether or not to use the memory from the KoboldAI GUI when generating text.',
    )
    use_story: Optional[bool] = Field(
        False,
        description='Whether or not to use the story from the KoboldAI GUI when generating text.',
    )
    use_userscripts: Optional[bool] = Field(
        False,
        description='Whether or not to use the userscripts from the KoboldAI GUI when generating text.',
    )
    use_world_info: Optional[bool] = Field(
        False,
        description='Whether or not to use the world info from the KoboldAI GUI when generating text.',
    )
    use_default_badwordsids: Optional[bool] = Field(
        None,
        description="Ban tokens that commonly worsen the writing experience for continuous story writing"
    )


class GenerationResult(BaseModel):
    text: str = Field(..., description='Generated output as plain text.')


class GensPerActionSetting(BaseModel):
    value: conint(ge=0, le=5)


class MaxContextLengthSetting(BaseModel):
    value: conint(ge=512, le=2048)


class MaxLengthSetting(BaseModel):
    value: conint(ge=1, le=512)


class MemorySetting(BaseModel):
    value: str


class Backend(Enum):
    KoboldAI_Old_Colab_Method = 'KoboldAI Old Colab Method'
    Huggingface = 'Huggingface'
    OpenAI = 'OpenAI'
    Horde = 'Horde'
    Huggingface_GPTQ = 'Huggingface GPTQ'
    KoboldAI_API = 'KoboldAI API'
    Read_Only = 'Read Only'
    GooseAI = 'GooseAI'
    ExLlama = 'ExLlama'
    Basic_Huggingface = 'Basic Huggingface'


class ModelSelection(BaseModel):
    backend: Optional[Backend] = None
    model: str = Field(
        ...,
        description='Hugging Face model ID, the path to a model folder (relative to the "models" folder in the KoboldAI root folder) or "ReadOnly" for no model',
    )


class NotFoundError(BaseModel):
    detail: BasicError


class NotImplementedError(BaseModel):
    detail: BasicError


class OutOfMemoryError(BaseModel):
    detail: BasicError


class RemoveBlankLinesSettings(BaseModel):
    value: bool


class RemoveSpecialCharactersSettings(BaseModel):
    value: bool


class SamplerFullDeterminismSetting(BaseModel):
    value: bool


class SamplerOrderSetting(BaseModel):
    value: List[int] = Field(..., min_items=6)


class SamplerSeedSetting(BaseModel):
    value: conint(ge=0, le=18446744073709551615)


class ServerBusyError(BaseModel):
    detail: BasicError


class SingleLineSettings(BaseModel):
    value: bool


class SoftPromptSetting(BaseModel):
    value: constr(pattern=r'^[^/\\]*$') = Field(
        ...,
        description='Soft prompt name, or a string containing only whitespace for no soft prompt. If using the GET method and no soft prompt is loaded, this will always be the empty string.',
    )


class SoftPromptsList(BaseModel):
    values: List[SoftPromptSetting] = Field(
        ..., description='Array of available softprompts.'
    )


class StoryChunk(BaseModel):
    num: int = Field(
        ...,
        description='Guaranteed to not equal the `num` of any other active story chunk. Equals 0 iff this is the first action of the story (the prompt).',
    )
    text: str = Field(..., description='The text inside this story chunk.')


class StoryChunkNum(BaseModel):
    value: int


class StoryChunkResult(BaseModel):
    result: StoryChunk


class StoryChunkSetText(BaseModel):
    value: constr(pattern=r'^(.|\n)*\S$')


class StoryChunkText(BaseModel):
    value: str


class StoryEmptyError(BaseModel):
    detail: BasicError


class StoryLoad(BaseModel):
    name: constr(pattern=r'^[^/\\]*$')


class StorySave(BaseModel):
    name: str #constr(pattern=r'^(?=.*\S)(?!.*[/\\]).*$')


class StoryTooShortError(BaseModel):
    detail: BasicError


class SubmissionInput(BaseModel):
    disable_input_formatting: Optional[bool] = Field(
        True,
        description='When enabled, disables all input formatting options, overriding their individual enabled/disabled states.',
    )
    frmtadsnsp: Optional[bool] = Field(
        None,
        description='Input formatting option. When enabled, adds a leading space to your input if there is no trailing whitespace at the end of the previous action.',
    )
    prompt: constr(pattern=r'^[\S\s]*\S[\S\s]*$') = Field(
        ..., description='This is the submission.'
    )


class TailFreeSamplingSetting(BaseModel):
    value: confloat(ge=0.0, le=1.0)


class TemperatureSamplingSetting(BaseModel):
    value: NonNegativeFloat


class TopASamplingSetting(BaseModel):
    value: confloat(ge=0.0)


class TopKSamplingSetting(BaseModel):
    value: conint(ge=0)


class TopPSamplingSetting(BaseModel):
    value: confloat(ge=0.0, le=1.0)


class TrimIncompleteSentencesSettings(BaseModel):
    value: bool


class TypicalSamplingSetting(BaseModel):
    value: confloat(ge=0.0, le=1.0)


class ValidationError(BaseModel):
    detail: Dict[str, List[str]]


class WorldInfoDepthSetting(BaseModel):
    value: conint(ge=1, le=5)


class WorldInfoEntriesUIDs(BaseModel):
    entries: List[conint(ge=-2147483648, le=2147483647)]


class WorldInfoEntry(BaseModel):
    comment: str = Field(
        ..., description='The comment/description/title for this world info entry.'
    )
    constant: bool = Field(
        ...,
        description='Whether or not constant mode is enabled for this world info entry.',
    )
    content: str = Field(..., description='The "What To Remember" for this entry.')
    key: str = Field(
        ...,
        description='Comma-separated list of keys, or of primary keys if selective mode is enabled.',
    )
    keysecondary: Optional[str] = Field(
        None,
        description='Comma-separated list of secondary keys if selective mode is enabled.',
    )
    selective: bool = Field(
        ...,
        description='Whether or not selective mode is enabled for this world info entry.',
    )
    uid: conint(ge=-2147483648, le=2147483647) = Field(
        ..., description='32-bit signed integer unique to this world info entry.'
    )


class WorldInfoFolder(BaseModel):
    entries: List[WorldInfoEntry]
    name: str = Field(..., description='Name of this world info folder.')
    uid: conint(ge=-2147483648, le=2147483647) = Field(
        ..., description='32-bit signed integer unique to this world info folder.'
    )


class WorldInfoFolderBasic(BaseModel):
    name: str = Field(..., description='Name of this world info folder.')
    uid: conint(ge=-2147483648, le=2147483647) = Field(
        ..., description='32-bit signed integer unique to this world info folder.'
    )


class WorldInfoFolderUIDs(BaseModel):
    entries: List[conint(ge=-2147483648, le=2147483647)]
    uid: conint(ge=-2147483648, le=2147483647) = Field(
        ..., description='32-bit signed integer unique to this world info folder.'
    )


class WorldInfoFolders(BaseModel):
    folders: List[WorldInfoFolderBasic]


class WorldInfoFoldersUIDs(BaseModel):
    folders: List[conint(ge=-2147483648, le=2147483647)]


class WorldInfoUIDs(BaseModel):
    entries: List[conint(ge=-2147483648, le=2147483647)]
    folders: List[WorldInfoFolderUIDs]


class BasicResult(BaseModel):
    result: BasicResultInner


class GenerationOutput(BaseModel):
    results: List[GenerationResult] = Field(
        ..., description='Array of generated outputs.'
    )


class Story(BaseModel):
    results: List[StoryChunk] = Field(
        ...,
        description='Array of story actions. The array is sorted such that actions closer to the end of this array are closer to the end of the story.',
    )


class WorldInfo(BaseModel):
    entries: List[WorldInfoEntry]
    folders: List[WorldInfoFolder]


class WorldInfoEntries(BaseModel):
    entries: List[WorldInfoEntry]
